[net]
batch=128
subdivisions=2
height=512
width=512
channels=3
learning_rate=0.001
momentum=0.9
decay=0.0005
seen=0

max_batches = 30000
burn_in=1000
policy=steps
steps=2500,5000,14000,18000,20000
scales=.5,.1,.1,.1,.1


#1 512x512
[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

#2 256x256
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

#3 128x128
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

#4 64x64
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

#5 32x32
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

#6 32x32
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

#7 32x32
[convolutional]
batch_normalize=1
filters=1
size=1
stride=1
pad=1
activation=logistic

[cost]

