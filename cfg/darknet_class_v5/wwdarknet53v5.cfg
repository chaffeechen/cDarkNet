[net]
# Training
batch=128
subdivisions=4

# Testing
#batch=1
#subdivisions=1

height=256
width=256
channels=3
min_crop=128
max_crop=448

burn_in=1000
learning_rate=0.02
#policy=poly
#power=4
policy=steps
steps=10000,20000,30000,40000
scales=.5,.5,.5,.5
max_batches=80000
momentum=0.9
decay=0.0005

angle=5
saturation = 1.5
exposure = 1.5
hue=.1
aspect=.75


[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

# Downsample

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=128
size=3
stride=2
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=256
size=3
stride=2
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[shortcut]
from=-3
activation=linear


[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[shortcut]
from=-3
activation=linear

# Downsample

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=2
pad=1
activation=leaky
learning_rate = 0.01


[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky
learning_rate = 0.01


[shortcut]
from=-3
activation=linear

[avgpool]

[convolutional]
filters=7
size=1
stride=1
pad=1
activation=linear

[bce]
groups=1
#class_weights = 1.0,1.0,0.33,0.1,0.1,1.0,0.33,1.0,1.0,1.0
#class_weights = 0.014,0.061,0.001,0.001,0.0048,0.0245,0.0129,1.0,0.161,0.263


